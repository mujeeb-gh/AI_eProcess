{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Data for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Age', 'Gender', 'Education Level', 'Job Title',\n",
       "       'Years of Experience', 'Salary', 'Age Group'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = \"Clean_Salary_Data.csv\"\n",
    "model_data = pd.read_csv(clean_data)\n",
    "model_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Selecting The Prediction Target or response variable\n",
    "y = model_data.Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choosing \"Features\" or predictors or predictor variables\n",
    "features = ['Age', 'Years of Experience']\n",
    "X = model_data[features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification or Regression\n",
    "### Since salary is a continuous variable, predicting it using a classification model such as KNN is not be the best choice. Instead, for predicting continuous variables like salary, we will consider using regression model such as Random Forest Regressor, Decision Tree and Linear Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since models' practical value come from making predictions on new data, we measure performance on data that wasn't used to build the model. The most straightforward way to do this is to exclude some data from the model-building process, and then use those to test the model's accuracy on data it hasn't seen before. This data is called validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18834.51563587788\n",
      "25638.358627236033\n",
      "0.7624243769300277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into training and tetsing/validation data, for both features and target\n",
    "# The split is based on a random number generator. Supplying a numeric value to\n",
    "# the random_state argument guarantees we get the same split every time we\n",
    "# run this script.\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state= 0)\n",
    "salary_model = DecisionTreeRegressor()\n",
    "salary_model.fit(train_X, train_y)\n",
    "\n",
    "# get predicted salaries on validation data\n",
    "val_prediction = salary_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, val_prediction))\n",
    "print(mean_squared_error(val_y, val_prediction, squared = False))\n",
    "print(salary_model.score(val_X, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mean absolute error for the in-sample data(before splitting data) was about 15,000 dollars, and accuracy of 83%. Out-of-sample it is more than 18,845 dollars, and accuracy of 76%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To overcome Overfitting and Underfitting problem, max_leaf_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    salary_model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    salary_model.fit(train_X, train_y)\n",
    "    val_prediction = salary_model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, val_prediction)\n",
    "    return(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 8  \t\t Mean Absolute Error:  19792\n",
      "Max leaf nodes: 80  \t\t Mean Absolute Error:  18538\n",
      "Max leaf nodes: 800  \t\t Mean Absolute Error:  18849\n",
      "Max leaf nodes: 8000  \t\t Mean Absolute Error:  18849\n"
     ]
    }
   ],
   "source": [
    "# compare MAE with differing values of max_leaf_nodes\n",
    "for max_leaf_nodes in [8, 80, 800, 8000]:\n",
    "    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n",
    "#     print(f\"Max leaf nodes: {max_leaf_nodes}  \\t\\t Mean Absolute Error:  {my_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Random Forest Model because of the Decision Tree's fallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18452.857811067504\n",
      "24802.707967354236\n"
     ]
    }
   ],
   "source": [
    "# X = model_data.drop(\"Salary\", axis=1).values\n",
    "# y = model_data[\"Salary\"].values\n",
    "\n",
    "# train_X, train_y, val_X, val_y = train_test_split(X, y, test_size = 0.3, random_state= 42)\n",
    "\n",
    "# categorical_cols = [cname for cname in train_X.columns if train_X[cname].dtype == \"object\"]\n",
    "\n",
    "# categorical_cols\n",
    "forest_model = RandomForestRegressor(random_state = 1)\n",
    "forest_model.fit(train_X, train_y)\n",
    "\n",
    "forest_pred = forest_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, forest_pred))\n",
    "print(mean_squared_error(val_y, forest_pred, squared = False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7776589561141818\n"
     ]
    }
   ],
   "source": [
    "print(forest_model.score(val_X, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can observe that the MAE using DecisionTree was 18827, and the MAE using Random Forest was 18452. A difference close to 400. Accuracy = 77%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23036.127101861824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "reg_model = LinearRegression()\n",
    "reg_model.fit(train_X, train_y)\n",
    "\n",
    "reg_pred= reg_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, reg_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7098435025632872\n",
      "23036.127101861824\n",
      "28333.839515751166\n"
     ]
    }
   ],
   "source": [
    "print(reg_model.score(val_X, val_y))\n",
    "print(mean_absolute_error(val_y, reg_pred))\n",
    "print(mean_squared_error(val_y, reg_pred, squared = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Accuracy = 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation for linear reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60547886 0.70841765 0.65889353 0.6584414  0.71095099 0.66926213]\n",
      "0.6685740942612282 0.03549924044166575\n",
      "[0.61209918 0.71063432]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits= 6, shuffle= True, random_state= 42)\n",
    "reg_model = LinearRegression()\n",
    "cv_results = cross_val_score(reg_model, X, y, cv= kf)\n",
    "# reported score is R squared\n",
    "print(cv_results)\n",
    "print(np.mean(cv_results), np.std(cv_results))\n",
    "# 95% confidence interval\n",
    "print(np.quantile(cv_results, [0.025, 0.975]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Regression to handle overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Supervised from GPT - To be read and reviewed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Mean Squared Error: 16707.24190263947\n",
      "Gradient Boosting - Mean Squared Error: 18145.477537377123\n",
      "Linear Regression - Mean Squared Error: 21422.44886478472\n",
      "Support Vector Regression - Mean Squared Error: 32482.074817681434\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assume you have a DataFrame 'model_data' with the specified features and the target variable 'Salary'\n",
    "\n",
    "# Separate features and target variable\n",
    "X = model_data[[\"Age\", \"Gender\", \"Years of Experience\", \"Education Level\", \"Job Title\"]]\n",
    "y = model_data[\"Salary\"]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = [\"Gender\", \"Education Level\", \"Job Title\"]\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_cols = [\"Age\", \"Years of Experience\"]\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numerical_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the models\n",
    "models = [\n",
    "    (\"Random Forest\", RandomForestRegressor(random_state=1)),\n",
    "    (\"Gradient Boosting\", GradientBoostingRegressor(random_state=1)),\n",
    "    (\"Linear Regression\", LinearRegression()),\n",
    "    (\"Support Vector Regression\", SVR(kernel=\"linear\")),\n",
    "    # Add more models as needed\n",
    "]\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models:\n",
    "    # Create the model pipeline\n",
    "    model_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "    model_pipeline.fit(train_X, train_y)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(val_y, model_pipeline.predict(val_X), squared= False)\n",
    "    print(f\"{model_name} - Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
